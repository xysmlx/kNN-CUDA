\documentclass[a4paper,12pt]{article}
\usepackage[top=2.54cm, bottom=2.54cm, left=3.18cm, right=3.18cm]{geometry}
\usepackage{ctex}
\usepackage[colorlinks,bookmarksnumbered=true,bookmarksopen=true,CJKbookmarks=true,linkcolor=red,anchorcolor=black,citecolor=black]{hyperref}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{ccmap}
\usepackage{listings}
\usepackage{color}
\usepackage{bbding}
\usepackage{url}
\usepackage{movie15}
\usepackage{booktabs,longtable}
\usepackage{mdwlist}
\usepackage{subfigure}
\usepackage{pifont}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{titlesec}
\usepackage[labelsep=space]{caption}
\usepackage{multirow,paralist}
\usepackage[title,titletoc]{appendix}
\usepackage[svgnames,x11names]{xcolor}
\usepackage[titles,subfigure]{tocloft}
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
%\renewcommand{\headrulewidth}{1pt}  %页眉线宽，设为0可以去页眉线
\lhead{\small{机器学习作业一}}
\chead{}
\rhead{\small{马凌霄(1501111302)，李奕(1501214394)}}
\lfoot{}
\cfoot{}
\rfoot{Page \thepage\ of \pageref{LastPage}}

  \def\CU@definezihao#1#2#3{
  \def#1{\fontsize{#2}{#3}\selectfont}}

\CU@definezihao{\zihaochu}{42}{50}
\CU@definezihao{\zihaoxiaochu}{36}{44}
\CU@definezihao{\zihaoyi}{28}{34}
\CU@definezihao{\zihaoer}{22}{26}
\CU@definezihao{\zihaoxiaoer}{18}{22}
\CU@definezihao{\zihaosan}{15.7}{19}
\CU@definezihao{\zihaosi}{14}{17}
\CU@definezihao{\zihaoxiaosi}{12}{14}
\CU@definezihao{\zihaowu}{10.5}{12}
\CU@definezihao{\zihaoxiaowu}{9}{11}
\CU@definezihao{\zihaoliu}{7.875}{9}
\CU@definezihao{\zihaoqi}{5.25}{6}

\titleformat{\section}{\zihaosi\bfseries}{$\S\;$\thesection}{1em}{}
\titleformat{\subsection}{\zihaoxiaosi\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\zihaoxiaosi\bfseries}{\thesubsubsection}{1em}{}

\usepackage{tabularx}
\newcommand{\PreserveBackslash}[1]{\let \temp =\\#1 \let \\ = \temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\usepackage[linesnumbered,ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{算法}

\theoremstyle{definition}
\newtheorem{defn}{定义}
\SetKwProg{Fn}{Function}{:}{end}

\usepackage{xcolor}

%\renewcommand{\figurename}{\zihaowu{Figure}}

\makeatletter
  \newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother

\usepackage{listings}
\lstset{tabsize=4, %
  frame=single, %把代码用带有阴影的框圈起来
  keywordstyle=\color[rgb]{0,0,1},
  commentstyle=\color[rgb]{0.133,0.545,0.133},
  stringstyle=\color[rgb]{0.627,0.126,0.941},
  %commentstyle=\color{red!50!green!50!blue!50},% 浅灰色的注释
  rulesepcolor=\color{red!20!green!20!blue!20},% 代码块边框为淡青色
  %keywordstyle=\color{blue!90}\bfseries, %代码关键字的颜色为蓝色，粗体
  showstringspaces=false,% 不显示代码字符串中间的空格标记
  stringstyle=\ttfamily, % 代码字符串的特殊格式
  keepspaces=true, %
  breakindent=22pt, %
  numbers=left,%左侧显示行号
  stepnumber=1,%
  numberstyle=\footnotesize, %行号字体用小号
  basicstyle=\footnotesize, %
  showspaces=false, %
  flexiblecolumns=true, %
  breaklines=true, % 对过长的代码自动换行
  breakautoindent=true,%
  breakindent=4em, %
  escapebegin=\begin{CJK*}{GBK}{hei},escapeend=\end{CJK*},
  aboveskip=1em, %代码块边框
  %% added by http://bbs.ctex.org/viewthread.php?tid=53451
  fontadjust,
  captionpos=t,
  framextopmargin=2pt,framexbottommargin=2pt,abovecaptionskip=-3pt,belowcaptionskip=3pt,
  xleftmargin=4em,xrightmargin=4em, % 设定listing左右的空白
  texcl=true,
  % 设定中文冲突，断行，列模式，数学环境输入，listing 数字的样式
  extendedchars=false,columns=flexible,mathescape=true
  % numbersep=-1em
}
\renewcommand{\figurename}{\zihaowu{图}}
\renewcommand{\tablename}{\zihaowu{表}}
\renewcommand{\abstractname}{\zihaoxiaosi{摘 \qquad 要}}
\renewcommand{\contentsname}{\centerline{\Large 目 \qquad 录}}
\renewcommand{\refname}{\centerline{\Large 参考文献}}
%\pagestyle{headings}
\begin{document}
\title{机器学习作业一}
\author{马凌霄(1501111302)\ \ \ \ \ \ \ \ \ \ 李奕(1501214394)}
\date{Adult Data Set}
\maketitle

\begin{abstract}
本文以UCI Machine Learning Repository的Adult Data Set为数据集，该数据集目前取得的最高分类准确率为$86.2\%$。我们训练k-Nearest Neighbors(KNN）, Logistics Regression(LR)，Support Vector Machine(SVM)三个分类器，分别取得了$81.98\%$，$81.23\%$，$83.25\%$的最好分类准确率。

首先，我们对数据进行了预处理，删除了不完整记录，名词性属性转换成数值型数值，将某些属性值离散化，删除某些属性项等；为了解决正负样本不均匀的情况，三个分类器都采用了样本权重修正，同时为了加快分类速度，KNN采用CUDA的GPU加速，取得了速度比CPU快了4倍的效果，对K值进行了参数择优；LR和SVM分类器调用了python中scikit-learn package, 并对LR的正则方法，正则系数和SVM的核函数，核函数系数，惩罚系数进行了参数择优。

实验结果表明，SVM在adult数据集取得了最好的效果，LR耗时最短，而KNN采用新颖的GPU加速，值得持续探索。但是由于时间有限，还有很多值得思考的地方，比如数据预处理，分类器训练速度优化等。
\end{abstract}

\newpage
\tableofcontents
\newpage
\section{背景介绍}
\subsection{小组成员和分工}
\begin{itemize}
  \item 马凌霄
  \begin{itemize}
    \item 学号：1501111302
    \item 院系：信息科学技术学院
    \item 邮箱：xysmlx@pku.edu.cn
    \item 分工：k-Nearest Neighbors，GPU加速kNN，数据预处理，数据可视化，实验报告\LaTeX排版
  \end{itemize}
  \item 李奕
  \begin{itemize}
    \item 学号：1501214394
    \item 院系：信息科学技术学院
    \item 邮箱：liyi193328@163.com
    \item 分工：Logistics Regression，Support Vector Machine，数据预处理
  \end{itemize}
\end{itemize}

\subsection{数据介绍：Adult Data Set}
本文选取UCI Machine Learning Repository中的Adult数据集 \footnote{\url{https://archive.ics.uci.edu/ml/datasets/Adult}}。

Adult数据集是根据某人的各种信息预测他的收入是否超过50,000/年。

Adult数据集有48842条记录。Adult数据集的每条记录有以下信息：
\begin{itemize}
\item age: continuous.
\item workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
\item fnlwgt: continuous.
\item education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
education-num: continuous.
\item marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
\item occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
\item relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
\item race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
\item sex: Female, Male.
\item capital-gain: continuous.
\item capital-loss: continuous.
\item hours-per-week: continuous.
\item native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad\&Tobago, Peru, Hong, Holand-Netherlands.
\end{itemize}

\subsection{测试环境}
测试使用了两台计算机及其GPU进行测试，硬件配置和编译器版本如表\ref{tab:computer1}-\ref{tab:compiler}所示。

\begin{table}[H]
  \centering
  \caption{\zihaowu 计算机1}
    \begin{tabular}{L{3cm}|L{10cm}}
    \toprule
    项目  & 详细信息\\
    \midrule
    CPU   & Core i7-2630QM (2.0GHz, 4 Cores, 6MB L3 Cache) \\
    内存    & 10GB DDR3 1333MHz \\
    测试所用磁盘 & 480GB Sandisk Extreme Pro SSD (Read: 550MB/s) \\
    操作系统  & Windows 10 Professional x64 \\
    \bottomrule
    \end{tabular}%
  \label{tab:computer1}%
\end{table}%

\begin{table}[H]
  \centering
  \caption{\zihaowu 计算机2}
    \begin{tabular}{L{3cm}|L{10cm}}
    \toprule
    项目  & 详细信息\\
    \midrule
    CPU   & Core i5-4460 (3.2GHz, 4 Cores, 6MB L3 Cache) \\
    内存    & 16GB DDR3 1600MHz \\
    测试所用磁盘 & 1TB Seagate 7200RPM HDD (Read: 121MB/s) \\
    操作系统  & Windows 10 Professional x64 \\
    \bottomrule
    \end{tabular}%
  \label{tab:computer2}%
\end{table}%

\begin{table}[H]
  \centering
  \caption{\zihaowu GPU1}
    \begin{tabular}{L{3cm}|L{10cm}}
    \toprule
    项目  & 详细信息\\
    \midrule
    型号   & nVIDIA GT550M \\
    流处理器    &  1480 MHz$\times$ 96 Cores  \\
    显存 & 2GB DDR3 900MHz \\
    显存位宽  & 128bit \\
    \bottomrule
    \end{tabular}%
  \label{tab:gpu1}%
\end{table}%

\begin{table}[H]
  \centering
  \caption{\zihaowu GPU2}
    \begin{tabular}{L{3cm}|L{10cm}}
    \toprule
    项目  & 详细信息\\
    \midrule
    型号   & nVIDIA GTX745 \\
    流处理器    &  1033 MHz$\times$ 384 Cores  \\
    显存 & 4GB DDR3 \\
    显存位宽  & 128bit \\
    \bottomrule
    \end{tabular}%
  \label{tab:gpu2}%
\end{table}%

\begin{table}[H]
  \centering
  \caption{\zihaowu 编译器版本}
    \begin{tabular}{L{3cm}|L{10cm}}
    \toprule
    项目  & 版本\\
    \midrule
    C/C++   & Microsoft Visual Studio 2013 \\
    C/C++    &  GNU C++ 4.8  \\
    Python & Python 3.4 \\
    GPU  & CUDA 7.5 \\
    \bottomrule
    \end{tabular}%
  \label{tab:compiler}%
\end{table}%

\subsection{评价指标}
本文选取精确度（Precision）、准确率（Accuracy）、召回率（Recall）、转移性（Specificity）、F-measure这五个指标作为评价指标。

假设原始样本中有两类，其中：
\begin{itemize}
  \item 总共有$P$个类别为1的样本，假设类别1为正例。
  \item 总共有$N$个类别为0的样本，假设类别0为负例。
\end{itemize}
经过分类后：
\begin{itemize}
  \item 有 $TP$个类别为1 的样本被系统正确判定为类别1，$FN$ 个类别为1 的样本被系统误判定为类别 0，显然有$P=TP+FN$；
  \item 有 $FP$ 个类别为0 的样本被系统误判断定为类别1，$TN$ 个类别为0 的样本被系统正确判为类别 0，显然有$N=FP+TN$；
\end{itemize}

\begin{defn}
精确度（Precision）：
$$P = \frac{TP}{(TP+FP)}$$
反映了被分类器判定的正例中真正的正例样本的比重。
\end{defn}

\begin{defn}
准确率（Accuracy）：
$$A = \frac{(TP + TN)}{(P+N)} = \frac{(TP + TN)}{(TP + FN + FP + TN)}$$
反映了分类器统对整个样本的判定能力――能将正的判定为正，负的判定为负。
\end{defn}

\begin{defn}
召回率(Recall)，也称为 True Positive Rate:
$$R = \frac{TP}{(TP+FN)} = 1 - \frac{FN}{T}$$
反映了被正确判定的正例占总的正例的比重 。
\end{defn}

\begin{defn}
转移性（Specificity），也称为 True Negative Rate：
$$S = \frac{TN}{(TN + FP)} = 1 - \frac{FP}{N}$$
明显的这个和召回率是对应的指标，只是用它在衡量类别0的判定能力。
\end{defn}

\begin{defn}
F-measure：
$$F = \frac{2 *  \mbox{召回率} \times  \mbox{准确率}}{(\mbox{召回率}+\mbox{准确率})}$$
\end{defn}

\section{数据处理}
\subsection{数据预处理}
由于仅使用原数据在各个算法中测试的结果均不太好，所以需要对数据进行预处理，根据多次调整数据预处理并且进行实验，得出最佳的数据预处理方法如下所示：
\begin{itemize}
  \item 将字符串标签转化为$0,1,2,3$这样的标签。
  \item 删去不完整的记录。
  \item 删去fnlwgt、education-num、marital-status、capital-gain、capital-loss、native-country。
  \item 对于hours-per-week
  \begin{itemize}
    \item 数值$\leq 39$，标记为0；
    \item 数值$>39$，标记为1；
  \end{itemize}
  \item 将age离散化：分为11组分别标记为$0-10$：$\leq 20$，$21-25$，$26-31$，$32-36$，$37-40$，$41-46$，$47-51$，$52-56$，$57-60$，$61-66$，$>66$
\end{itemize}

经过预处理后，数据集变为：45222条记录，每条记录有8个信息和1个最终标记。数据样例如图\ref{fig:adult}所示。

\begin{figure}[H]
  \centering
  \includegraphics[width=350 pt]{adult.jpg}
  \caption{\zihaowu 预处理后的Adult数据集}\label{fig:adult}
\end{figure}

\subsection{训练集与测试集划分：10-fold cross-validation}
本文使用10折交叉验证（10-fold cross-validation）的方法进行训练集和测试集的划分：将数据集按照$9:1$划分为训练集和测试集，重复10次，每次选取不同的$10\%$作为测试集，其余$90\%$作为测试集，取10次的平均值作为最终的测试结果。

\section{k-Nearest Neighbors}

\subsection{算法简介}
k-Nearest Neighbors算法的思想是：如果一个样本在特征空间中的$k$个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别。

k-Nearest Neighbors算法的流程如算法\ref{alg:knn}所示。

\begin{algorithm}[H]
\caption{k-Nearest Neighbors}
\label{alg:knn}
\KwIn{训练数据集$D$, 测试数据集$T$, 阈值$K$}
\KwOut{预测的标签}
\Fn{kNN($D$,$T$,$K$)}
{
    初始化距离为最大值\;
    \For{$t\in T$}
    {
        \For{$d\in D$}
        {
            计算$t$与$d\in D$的距离$dis$\;
            sort(dis)\;
            得到目前$K$个最临近样本中的最大距离$maxdis$\;
            \If{$dis<maxdis$}{将该训练样本作为$K$-最近邻样本\;}
        }
        统计$K$-最近邻样本中每个类标号出现的次数\;
        选择出现频率最大的类标号作为未知样本的类标号\;
    }
}
\end{algorithm}

使用的距离度量为欧式距离，如式\ref{equ:dis}所示。

\begin{equation}\label{equ:dis}
dis(A,B)=\sqrt{\sum^{n}_{i=1}{(A_i-B_i)}^2}
\end{equation}

\subsection{算法实现}
\begin{itemize}
  \item 语言：C++
  \item 使用的库：无
\end{itemize}

k-Nearest Neighbors的各个函数定义如下所示：
\begin{lstlisting}[language=c++]
class KNN
{
public:
	static const int maxCol = 10;
	static const int maxRow = 46010;

	static const int testNum = 4000; // Data number for test

public:
	struct Node
	{
		double data[maxCol];
		string label;
	};

	struct CMP
	{
		bool operator ()(const prid &A, const prid &B) const
		{
			return A.second < B.second;
		}
	};

public:
	void init(int _k, int _row, int _col, string path); // init
	void input(); // Input data
	void ZScoreNorm(); // Z-Score Norm
	void MaxMinNorm(); // Max min Norm
	double dis(const Node &A, const Node &B); // Calculate distance between A and B
	void CalDis(); // Calculate distance between dataTest and dataTrain
	string MaxFreqLabel(); // Calculate freq label
	void knn(); // Run knn
	void debug(); // For debug

public:
	ifstream fin;
	string filepath;

	int therK;
	int row;
	int col;

	Node dataSet[maxRow];
	Node dataTest;

	vector<prid> vec;
	map<string, int> mp;
};
\end{lstlisting}

\paragraph{变量简介}
\begin{itemize}
  \item struct Node：存储一条记录的结构体。
  \begin{itemize}
    \item $double\ data[maxCol]$：存储一条记录的每个信息。
    \item $string\ label$：存储该记录的标签。
  \end{itemize}
  \item $ifstream\ fin$：文件输入，用于读取文件。
  \item $string\ filepath$：输入文件的路径。
  \item $int\ therK$：kNN的阈值$K$。
  \item $int\ row$：记录的条数（行数）。
  \item $int\ col$：记录的列数。
  \item $Node\ dataSet[maxRow]$：存储整个记录。
  \item $vector<prid>\ vec$：存储$int,double$的$pair$，$int$为该记录在$dataSet$的下标，$double$为该记录与测试数据的距离。
  \item $map<string, int>\ mp$：用于统计某标签出现的次数。
\end{itemize}

\paragraph{函数简介}
\begin{itemize}
  \item $void\ init(int\ \_k, int\ \_row, int\ \_col, string\ path)$：初始化所有变量
  \begin{itemize}
    \item 输入：kNN的阈值$\_k$，数据集的行数$\_row$，数据集的列数$\_col$，数据文件路径$path$
    \item 输出：无，初始化好的变量
  \end{itemize}
  \item $void\ input()$：使用ifstream从数据文件读取数据
  \item $void\ ZScoreNorm()$：Z-Score标准化
  \item $void\ MaxMinNorm()$：Max-Min标准化
  \item $double\ dis(const\ Node\ \&A, const\ Node\ \&B)$：按照式\ref{equ:dis}计算数据记录A和B的距离
  \begin{itemize}
    \item 输入：数据记录A，数据记录B
    \item 输出：A和B的距离
  \end{itemize}
  \item $void\ CalDis()$：计算测试数据记录与所有训练数据记录的距离
  \begin{itemize}
    \item 输入：测试数据记录，训练数据记录集合
    \item 输出：测试数据记录与所有训练数据记录的距离，用vector输出
    \item 流程：遍历一遍训练数据记录集合，调用dis函数计算所有的距离
  \end{itemize}
  \item $string\ MaxFreqLabel()$：找出出现次数最高的label，
  \begin{itemize}
    \item 输入：测试数据记录，训练数据记录集合
    \item 输出：预测得到的标签
    \item 流程：
    \begin{enumerate}
      \item 调用$CalDis()$计算未知样本和每个训练样本的距离$dis$
      \item 得到目前$K$个最临近样本中的最大距离$maxdis$
      \item 如果$dis$小于$maxdis$，则将该训练样本作为$K-$最近邻样本
      \item 对于每个训练样本，执行1-3步
      \item 统计$K-$最近邻样本中每个类标号出现的次数
      \item 选择出现次数最大的作为预测标签，return标签
    \end{enumerate}
  \end{itemize}
  \item $void\ knn()$：kNN主控
  \begin{itemize}
    \item 流程：
    \begin{enumerate}
      \item 对于一个测试样本，调用$MaxFreqLabel()$
      \item 判断正确与否，统计TP、TN、FP、FN
      \item 对于每个测试样本，执行1-2步
      \item 计算精确度、准确率、召回率、转移性、F-measure
    \end{enumerate}
  \end{itemize}
\end{itemize}

\subsection{GPU加速机器学习 \& CUDA介绍}
随着数据集的增大，目前已有的机器学习算法训练大规模数据所需时间越来越多。而且对于较为复杂的机器学习算法，训练的时间也非常高。由于机器学习算法有着大量迭代等适合并行化的过程而且一般GPU的核心数能达到3072(nVIDIA TITAN X)而主流CPU核心数为4，所以可以使用GPU的大量计算核心加速机器学习算法。

CUDA（Compute Unified Device Architecture，统一计算架构）是由nVIDIA所推出的一种集成技术，是该公司对于GPGPU的正式名称。通过CUDA，用户可利用nVIDIA的显卡进行GPU计算。CUDA可以兼容OpenCL或者自家的C-编译器。无论是CUDA C-语言或是OpenCL，指令最终都会被驱动程序转换成PTX代码，交由显示核心计算。

使用CUDA进行GPU加速的数据交换主要通过计算机内存和GPU显存之间互相传输数据。所以GPU加速的过程即为：
\begin{enumerate}
  \item 把数据从内存传输到显存
  \item 让GPU对显存中的数据进行计算，把结果写入显存
  \item 将计算结果从显存中传输到内存
\end{enumerate}

\subsection{GPU加速的算法实现}
在这里，GPU加速主要运用于测试样本与训练样本集合的距离计算。设计思想是：把测试样本与训练样本集合的距离计算交给GPU计算，让GPU每个计算核心计算一个测试样本与一个训练样本的距离，再输出到显存，传输给内存。

GPU计算距离的函数设计如下所示：
\begin{lstlisting}[language=c++]
__global__ void DisKernel(double *traindata, double *testdata, double *dis, int pitch, int N, int D)
{
	int tid = blockIdx.x;
	if (tid < N)
	{
		double temp = 0;
		double sum = 0;
		for (int i = 0; i < D; i++)
		{
			temp = *((double*)((char*)traindata + tid * pitch) + i) - testdata[i];
			sum += temp * temp;
		}
		dis[tid] = sum;
	}
}
\end{lstlisting}

其余部分和标准kNN的设计一样

\subsection{实验}
\subsubsection{参数$K$选取实验}
$K$选取3-15，统计精确度、准确率、召回率、转移性、F-measure，如表\ref{tab:knnselk}和图\ref{fig:knnselk}所示。
\begin{itemize}
  \item $K=6$：此时召回率为0.691445最高，F-measure为0.748997最高。
  \item $K=12$：此时准确率为0.81975最高。召回率和F-measure也比较高。设置为最优参数。
\end{itemize}

% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[H]
  \centering
  \caption{\zihaowu 参数$K$选取实验}
    \begin{tabular}{L{1cm}R{2cm}R{2cm}R{2cm}R{2cm}R{2cm}}
    \toprule
    k     & 精确度   & 准确率   & 召回率   & 转移性   & F-measure \\
    \midrule
    3     & 0.585075 & 0.80225 & 0.611227 & 0.862739 & 0.69383 \\
    4     & 0.473632 & 0.80975 & 0.672316 & 0.839307 & 0.734661 \\
    5     & 0.597015 & 0.8065 & 0.619195 & 0.866381 & 0.700544 \\
    6     & 0.490547 & 0.817 & 0.691445 & 0.844235 & 0.748997 \\
    7     & 0.581095 & 0.81825 & 0.65618 & 0.86463 & 0.728307 \\
    8     & 0.514428 & 0.81875 & 0.685676 & 0.849661 & 0.746328 \\
    9     & 0.591045 & 0.81825 & 0.652747 & 0.86699 & 0.726188 \\
    10    & 0.508458 & 0.81425 & 0.672368 & 0.847531 & 0.736539 \\
    11    & 0.587065 & 0.81725 & 0.651214 & 0.865869 & 0.724845 \\
    12    & 0.528358 & 0.81975 & 0.682519 & 0.852886 & 0.744867 \\
    13    & 0.573134 & 0.8155 & 0.650847 & 0.862279 & 0.72393 \\
    14    & 0.528358 & 0.81375 & 0.662095 & 0.851782 & 0.73013 \\
    15    & 0.569154 & 0.81375 & 0.647059 & 0.86104 & 0.720894 \\
    \bottomrule
    \end{tabular}%
  \label{tab:knnselk}%
\end{table}%


\begin{figure}[H]
  \centering
  \includegraphics[width=300 pt]{knnselk.eps}
  \caption{\zihaowu 参数$K$选取实验}\label{fig:knnselk}
\end{figure}
\subsubsection{最优参数下的实验}
由上一小节可知，选取最优的$K$值为12，当$K=12$时，准确率、召回率、F-measure、精确率如表\ref{tab:knn}所示。

% Table generated by Excel2LaTeX from sheet 'final'
\begin{table}[H]
  \centering
  \caption{\zihaowu kNN最优参数下的实验}
    \begin{tabular}{L{2cm}R{1.5cm}R{1.5cm}R{2cm}R{1.5cm}R{2.5cm}}
    \toprule
    数据集    & 准确率   & 召回率   & F-measure & 精确率   & 最优参数 \\
    \midrule
    原始数据   & 0.77025 & 0.589212 & 0.667676 & 0.282587 & k=7 \\
    处理数据   & 0.81975 & 0.682519 & 0.744867 & 0.528358 & k=12 \\
    \bottomrule
    \end{tabular}%
  \label{tab:knn}%
\end{table}%

\subsubsection{GPU加速的实验}
使用表\ref{tab:computer1}-\ref{tab:compiler}所示的环境，进行2台计算机和2个GPU的对比实验。测得计算所用时间如图\ref{fig:knntime}所示，测试发现GPU对kNN的加速程度非常大，即使是入门级的GPU nVIDIA GT550M，所需的时间仅为Core-i7计算时间的$1/4$。所以GPU能够非常明显地对kNN进行加速。

\begin{figure}[H]
  \centering
  \includegraphics[width=300 pt]{knntime.eps}
  \caption{\zihaowu GPU加速的实验（单位：秒(s)）}\label{fig:knntime}
\end{figure}

\section{Logistics Regression}

\subsection{算法实现}
\begin{itemize}
  \item 语言：Python
  \item 使用的库：scikit-learn, pylearn2
\end{itemize}

\paragraph{scikit-learn简述}Logistics回归在scikit-learn中的函数介绍
\begin{itemize}
  \item 函数默认参数：
\begin{lstlisting}[language=python]
class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)
\end{lstlisting}
  \item 重要参数说明：
  \begin{itemize}
    \item $penalty$：表示正则方式，可用的方式是"l1"或者"l2"分别对应了l1和l2正则方式
    \item $C$：逆正则系数，C值越小，代表正则强度越大；反之，越小
    \item $class\_weight$: 类权重，如果设置为”balanced”,则类的权重按照正负样本比例，也可以用户自己设定
    \item $n\_jobs$: 并行线程数目
  \end{itemize}
\end{itemize}

Logistics Regression核心代码如下所示：
\begin{lstlisting}[language=python]
log_tuned_parameters = [{'penalty': ['l1'],'C': [1, 5, 10, 100]},
	                    {'penalty': ['l2'], 'C': [1, 5 ,10, 100]}]
	print("logistic regression:")
	logS = time.clock()
	logisticRegression = LogisticRegression()
	score = "f1_micro"
	cv =ShuffleSplit(len(trainVec),3,test_size=0.1,random_state=10)
	clf = GridSearchCV(logisticRegression, log_tuned_parameters, cv=cv, n_jobs =1,scoring='%s' % score)
	clf.fit(trainVec,trainLabel)
	print(clf.best_params_)
	predict = clf.predict(testVec)
	accuracy = metrics.accuracy_score(testLabel,predict)
	recall =metrics.recall_score(testLabel, predict,pos_label=None, average='micro')
	print("accuracy,recall: ",accuracy,recall)
	print(metrics.classification_report(testLabel,predict))
	print("confusion_matrix:")
	cm = metrics.confusion_matrix(testLabel,predict)
	print(cm)
	logE = time.clock()
	print("Logistic Regression time: {0}s".format(logE-logS) )
\end{lstlisting}

\paragraph{代码解释}
\begin{itemize}
  \item log\_tuned\_parameters：表示的参数的备选项，在分类时会尝试每一个可能参数的每一个取值情况，并选择给定条件（这里实际上是f1\_score或者F-measure值）的最优参数
  \item trainVec, trainLabel表示的是训练数据的样本向量和标签；testVec, testLabel表示的是测试数据向量和标签
  \item GridSearchCV函数的目的是在训练数据时自动搜索出给定目标下多个参数的最优参数，它的第一个参数是定义好的分类器，第二个参数是需要调整的参数，它是以字典(dict)的形式传入的，cv表示对训练数据采用交叉验证的方式，可以是整数，也可以是定义好的交叉验证类，此时我们传入的是一个交叉验证方式（首先对数据进行shuffle，按照9:1数据分割，迭代10次）；n\_jobs 是并行运算的线程数目；scoring是在选择参数的标准，设定的scoring是f1\_score，表示在候选参数中选择使得f1\_score最大的参数组合
  \item clf.fit传入的是训练样本和标签（它会自动训练出最优的参数和分类器），然后用clf.predicty作用在predict数据上，得到predict的label
  \item metrics.accuracy\_score输入是测试数据中准确的标签（label）和预测的标签，得到的是相应的准确率；metrics.recall\_score得到是召回率; metrics.classification\_report 得到的是precision, recall, f1\_score三个指标；cm 表达的是它们的混淆矩阵
\end{itemize}

\subsection{实验}
准确率，召回率，F-measure，精确率都是根据正负样本量平均的结果，如表\ref{tab:logistics}所示。

\begin{table}[H]
  \centering
  \caption{\zihaowu Logistics Regression最优参数下的实验}
    \begin{tabular}{L{2cm}R{1.5cm}R{1.5cm}R{2cm}R{1.5cm}R{2.5cm}}
    \toprule
    数据集    & 准确率   & 召回率   & F-measure & 精确率   & 最优参数 \\
    \midrule
    原始数据  & 0.772937 & 0.789676 & 0.7566677 & 0.768876 & C=0.5 penalty=l1 \\
    处理数据  & 0.814563 & 0.812379 & 0.824573 & 0.815431 & C=1 penalty=l1 \\
    \bottomrule
    \end{tabular}%
  \label{tab:logistics}%
\end{table}%

\section{Support Vector Machine}

\subsection{算法实现}
\begin{itemize}
  \item 语言：Python
  \item 使用的库：scikit-learn, pylearn2
\end{itemize}

SVM在scikit-learn中的函数介绍：
\begin{itemize}
  \item 函数默认参数：
\begin{lstlisting}[language=python]
class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=None)
\end{lstlisting}
  \item 重要参数说明：
  \begin{itemize}
    \item $C$：错误项的惩罚系数
    \item $Kernel$: 核函数，'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' 或者自己设置的可调用的核函数
    \item $class\_weight$: 类权重，如果设置为"balanced",则类的权重按照正负样本比例，也可以用户自己设定
    \item $gamma$: 核函数系数
    \item $n\_jobs$: 并行线程数目	
  \end{itemize}
\end{itemize}

SVM核心代码如下所示：
\begin{lstlisting}[language=python]
svm_tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.5],'C': [1, 5, 10, 100]},{'kernel': ['linear'], 'gamma': [0.5],'C': [1, 5, 10, 100]}]
	print("\nsvm:")
	svmTS = time.clock()
	svmKernelway = 'rbf'
	svrrbf = SVC(kernel=svmKernelway,C=10,gamma=0.5)
	score = "f1_micro"
	cv =ShuffleSplit(len(trainVec),2,test_size=0.1,random_state=10)
	clf =  GridSearchCV(SVC(C=1), svm_tuned_parameters, cv=cv, n_jobs = 2, scoring='%s' % score)
	clf.fit(trainVec,trainLabel)
	print(clf.best_params_)
	testLabel = clf.predict(testVec)
	accuracy = metrics.accuracy_score(testLabel,predict)
	recall = metrics.recall_score(testLabel,predict,pos_label=None,average='micro')
	print("accuracy,recall: ",accuracy,recall)
	print(metrics.classification_report(testLabel,predict))
	print("confusion_matrix:")
	cm = metrics.confusion_matrix(testLabel,predict)
	print(cm)
	svmTE = time.clock()
	print("svm time: {0}s".format(svmTE-svmTS) )
\end{lstlisting}

\paragraph{代码解释}在scikit-learn中的svm分类器的调用和logistics回归调用结构几乎一致，只是在参数类别含义不一致，因此代码和logistics回归代码相比较，主要区别除了分类器名字的差别外，其余就是参数设置，svm\_tuned\_parameters：此处我们对gamma取定为0.5或者1，C值设定为1,5,10,100，在这些参数组合中选择使得f1\_score最大的参数组合。

\subsection{实验}
\paragraph{问题}
在参数训练过程中，由于数据切割不均匀导致训练样本或者测试样本中正负样本比例不均匀的情况，这样会产生某一类的召回率特别低（有时还会出现0的情况）,如果仅仅只看正负样本召回率的平均值，就会错误地认为分类效果还不错，但是这种情况是畸形的。

\paragraph{解决办法}
设定class\_weight参数为"balanced"或者自己设定一个正负样本的权重值，让样本多的一方权重稍微低一些，样本少的一方权重高一些，而balanced参数是根据正负样本的数量来设定权重的，我们的程序里面class\_weight设定就是balanced。

准确率，召回率，F-measure，精确率都是根据正负样本量平均的结果，如表\ref{tab:svm}所示。

\begin{table}[H]
  \centering
  \caption{\zihaowu SVM最优参数下的实验}
    \begin{tabular}{L{2cm}R{1.5cm}R{1.5cm}R{2cm}R{1.5cm}R{2.5cm}}
    \toprule
    数据集    & 准确率   & 召回率   & F-measure & 精确率   & 最优参数 \\
    \midrule
    原始数据  & 0.802537 & 0.799656 & 0.806777 & 0.818876 & C=1 gamma=0.5 kernel=rbf \\
    处理数据  & 0.824663 & 0.832479 & 0.835473 & 0.834431 & C=10 gamma=0.5 kernel=rbf \\
    \bottomrule
    \end{tabular}%
  \label{tab:svm}%
\end{table}%

\section{综合测试对比}
选取准确率，召回率，F-measure，精确率作为评价指标，kNN、Logistics Regression和SVM的测试结果如表\ref{tab:alltest}和图\ref{fig:alltest}所示。

k-Nearest Neighbors(KNN）, Logistics Regression(LR)，Support Vector Machine(SVM)三个分类器，分别取得了$81.98\%$，$81.23\%$，$83.25\%$的最好分类准确率。

查阅文献得知，该数据集目前最高的分类准确率为$86.2\%$。

\begin{table}[H]
  \centering
  \caption{\zihaowu 综合测试对比}
    \begin{tabular}{L{1.5cm}R{1.5cm}R{1.5cm}R{2cm}R{1.5cm}R{2.5cm}}
    \toprule
    算法    & 准确率   & 召回率   & F-measure & 精确率   & 最优参数 \\
    \midrule
    kNN  & 0.81975 & 0.682519 & 0.744867 & 0.528358 & k=12 \\
    Logistics  & 0.814563 & 0.812379 & 0.824573 & 0.815431 & C=1 penalty=l1 \\
    SVM  & 0.824663 & 0.832479 & 0.835473 & 0.834431 & C=10 gamma=0.5 kernel=rbf \\
    \bottomrule
    \end{tabular}%
  \label{tab:alltest}%
\end{table}%

\begin{figure}[H]
  \centering
  \includegraphics[width=300 pt]{alltest.eps}
  \caption{\zihaowu 综合测试对比}\label{fig:alltest}
\end{figure}

\section{总结}
\begin{enumerate}
  \item 第一次用实际的数据做分类器训练，对数据预处理或者特征选择有更好的认识：对各种特征的选择或者处理方法只要合理，能够解释，都可以尝试。
  \item 通过KNN的GPU实验，学了CUDA编程的基本知识，对并行程序有更好的认识。
  \item 了解熟悉了scikit-learn包，学会了调整参数的方法。
  \item 由于时间有限，有好多临时结果没有保留下来，重新做一遍又非常耗时，因此只有一个总体的结果，非常遗憾：以后在编写程序过程中注意中间结果的保留！
  \item 训练过程是一个复杂的过程，即使一个简单的分类器也有很多值得思考的地方。
\end{enumerate}


\newpage

\begin{thebibliography}{100}
\bibitem[1]{bib:adult}UCI Machine Learning Repository. Adult Data Set. \url{https://archive.ics.uci.edu/ml/datasets/Adult}.
\bibitem[2]{bib:adultref}Kohavi R. Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid[C]//KDD. 1996: 202-207.
\bibitem[3]{bib:lihang}李航. 统计学习方法[J]. 2012.
\bibitem[4]{bib:cuda}nVIDIA. CUDA. \url{https://developer.nvidia.com/cuda-downloads}.
\end{thebibliography}


\newpage

%\appendix
%\section{代码清单\&如何编译执行}
%\subsection{k-Nearest Neighbors}
%\subsubsection{标准版(kNN.cpp)}
%\begin{lstlisting}[language=c++]
%// #pragma comment(linker, "/STACK:102400000,102400000")
%#include <cstdio>
%#include <iostream>
%#include <cstring>
%#include <string>
%#include <cmath>
%#include <set>
%#include <list>
%#include <map>
%#include <iterator>
%#include <cstdlib>
%#include <vector>
%#include <queue>
%#include <ctime>
%#include <stack>
%#include <algorithm>
%#include <functional>
%#include <ctime>
%#include <fstream>
%using namespace std;
%typedef long long ll;
%const double INF = 1e30;
%typedef pair<int, double> prid;
%
%class KNN
%{
%public:
%	static const int maxCol = 10;
%	static const int maxRow = 46010;
%
%	static const int testNum = 4000; // Data number for test
%
%public:
%	struct Node
%	{
%		double data[maxCol];
%		string label;
%	};
%
%	struct CMP
%	{
%		bool operator ()(const prid &A, const prid &B) const
%		{
%			return A.second < B.second;
%		}
%	};
%
%public:
%	void init(int _k, int _row, int _col, string path); // init
%	void input(); // Input data
%	void ZScoreNorm(); // Z-Score Norm
%	void MaxMinNorm(); // Max min Norm
%	double dis(const Node &A, const Node &B); // Calculate distance between A and B
%	void CalDis();
%	string MaxFreqLabel(); // Calculate freq label
%	void knn(); // Run knn
%	void debug(); // For debug
%
%public:
%	ifstream fin;
%	string filepath;
%
%	int therK;
%	int row;
%	int col;
%
%	Node dataSet[maxRow];
%	Node dataTest;
%
%	vector<prid> vec;
%	map<string, int> mp;
%};
%
%void KNN::init(int _k, int _row, int _col, string path)
%{
%	therK = _k;
%	row = _row;
%	col = _col;
%	filepath = path;
%
%	cout << therK << " " << row << " " << col << endl;
%}
%
%void KNN::input()
%{
%	fin.open(filepath);
%	for (int i = 0; i < row; i++)
%	{
%		for (int j = 0; j < col; j++)
%			fin >> dataSet[i].data[j];
%		fin >> dataSet[i].label;
%	}
%	fin.close();
%	// debug();
%
%	MaxMinNorm();
%	// ZScoreNorm();
%}
%
%void KNN::ZScoreNorm()
%{
%	for (int j = 0; j < col; j++)
%	{
%		double avg = 0;
%		for (int i = 0; i < row; i++)
%			avg += dataSet[i].data[j];
%		avg /= (double)row;
%
%		double sig = 0;
%		for (int i = 0; i < row; i++)
%			sig += (dataSet[i].data[j] - avg) * (dataSet[i].data[j] - avg);
%		sig /= (double)row;
%		sig = sqrt(sig);
%
%		for (int i = 0; i < row; i++)
%			dataSet[i].data[j] = (dataSet[i].data[j] - avg) / sig;
%	}
%}
%
%void KNN::MaxMinNorm()
%{
%	for (int j = 0; j < col; j++)
%	{
%		double maxx = max(dataSet[0].data[j], dataSet[1].data[j]);
%		double minx = min(dataSet[0].data[j], dataSet[1].data[j]);
%		for (int i = 0; i < row; i++)
%		{
%			//if (dataSet[i].data[j] > maxx) maxx = dataSet[i].data[j];
%			//else if (dataSet[i].data[j] < minx) minx = dataSet[i].data[j];
%			maxx = max(maxx, dataSet[i].data[j]);
%			minx = min(minx, dataSet[i].data[j]);
%		}
%		for (int i = 0; i < row; i++)
%			dataSet[i].data[j] = (double)(dataSet[i].data[j] - minx) / (double)(maxx - minx);
%	}
%}
%
%double KNN::dis(const Node &A, const Node &B)
%{
%	double ret = 0;
%	for (int i = 0; i < col; i++)
%		ret += (A.data[i] - B.data[i]) * (A.data[i] - B.data[i]);
%	return sqrt(ret);
%}
%
%void KNN::CalDis()
%{
%	vec.clear();
%	for (int i = testNum; i < row; i++)
%		vec.push_back(make_pair(i, dis(dataTest, dataSet[i])));
%}
%
%string KNN::MaxFreqLabel()
%{
%	CalDis();
%	sort(vec.begin(), vec.end(), CMP());
%
%	mp.clear();
%	for (int i = 0; i < therK; i++)
%		mp[dataSet[vec[i].first].label]++;
%
%	string ret;
%	int cnt = 0;
%	for (auto ite = mp.begin(); ite != mp.end(); ite++)
%	{
%		if (ite->second > cnt)
%		{
%			cnt = ite->second;
%			ret = ite->first;
%		}
%	}
%
%	return ret;
%}
%
%void KNN::knn()
%{
%	cout << "Test data num: " << testNum << endl;
%
%	int cntTP = 0, cntFP = 0, cntTN = 0, cntFN = 0;
%	for (int i = 0; i < testNum; i++)
%	{
%		dataTest = dataSet[i];
%		string tmp = MaxFreqLabel();
%		if (tmp == dataTest.label)
%		{
%			if (dataTest.label == "1") cntTP++;
%			else cntTN++;
%		}
%		if (tmp != dataTest.label)
%		{
%			if (dataTest.label == "1") cntFP++;
%			else cntFN++;
%		}
%	}
%
%	double Prec = (double)cntTP / (double)(cntTP + cntFP);
%	double Accu = (double)(cntTP + cntTN) / (double)(testNum);
%	double Recall = (double)cntTP / (double)(cntTP + cntFN);
%	double S = (double)cntTN / (double)(cntTN + cntFP);
%	double F = 2 * Recall * Accu / (Recall + Accu);
%
%	cout << Prec << " " << Accu << " " << Recall << " " << S << " " << F << endl;
%}
%
%void KNN::debug()
%{
%	ofstream fout;
%	fout.open("debug.txt");
%	for (int i = 0; i < row; i++)
%	{
%		for (int j = 0; j < col; j++)
%			fout << dataSet[i].data[j] << "\t";
%		fout << dataSet[i].label << endl;
%	}
%}
%
%KNN knn;
%
%void init()
%{
%	knn.init(7, 45222, 8, "allTypeC.txt");
%}
%void input()
%{
%	knn.input();
%}
%void debug()
%{
%	//
%}
%void solve()
%{
%	clock_t st, ed;
%	st = clock();
%	knn.knn();
%	ed = clock();
%	cout << "Time: " << ed - st << endl;
%}
%void output()
%{
%	//
%}
%int main()
%{
%	init();
%	input();
%	solve();
%	output();
%
%	return 0;
%}
%\end{lstlisting}
%\subsubsection{GPU加速版(kNN-CUDA.cu)}
%\begin{lstlisting}[language=c++]
%// #pragma comment(linker, "/STACK:102400000,102400000")
%#include <cstdio>
%#include <iostream>
%#include <cstring>
%#include <string>
%#include <cmath>
%#include <set>
%#include <list>
%#include <map>
%#include <iterator>
%#include <cstdlib>
%#include <vector>
%#include <queue>
%#include <ctime>
%#include <stack>
%#include <algorithm>
%#include <functional>
%#include <ctime>
%#include <fstream>
%using namespace std;
%typedef long long ll;
%const double INF = 1e30;
%typedef pair<int, double> prid;
%
%class KNN
%{
%public:
%	static const int maxCol = 10;
%	static const int maxRow = 46010;
%
%	static const int testNum = 4000; // Data number for test
%
%public:
%	struct Node
%	{
%		double data[maxCol];
%		string label;
%	};
%
%	struct CMP
%	{
%		bool operator ()(const prid &A, const prid &B) const
%		{
%			return A.second < B.second;
%		}
%	};
%
%public:
%	void init(int _k, int _row, int _col, string path); // init
%	void input(); // Input data
%	void ZScoreNorm(); // Z-Score Norm
%	void MaxMinNorm(); // Max min Norm
%	double dis(const Node &A, const Node &B); // Calculate distance between A and B
%	void CalDis();
%	string MaxFreqLabel(); // Calculate freq label
%	void knn(); // Run knn
%
%	void CUDAInit(); // Init CUDA
%	//static __global__ void DisKernel(double *traindata, double *testdata, double *dis, int pitch, int N, int D);
%
%
%	void debug(); // For debug
%
%public:
%	ifstream fin;
%	string filepath;
%
%	int therK;
%	int row;
%	int col;
%
%	Node dataSet[maxRow];
%	Node dataTest;
%
%	vector<prid> vec;
%	map<string, int> mp;
%
%	// Data for CUDA
%	double trainmtx[maxRow][maxCol];
%	int trainRow;
%	double *GTrainData;
%	size_t pitch_d;
%	size_t pitch_h;
%	double *GTestData;
%	double *GDis;
%	double *distance;
%};
%
%void KNN::init(int _k, int _row, int _col, string path)
%{
%	therK = _k;
%	row = _row;
%	col = _col;
%	filepath = path;
%
%	cout << therK << " " << row << " " << col << endl;
%}
%
%void KNN::input()
%{
%	fin.open(filepath);
%	for (int i = 0; i < row; i++)
%	{
%		for (int j = 0; j < col; j++)
%			fin >> dataSet[i].data[j];
%		fin >> dataSet[i].label;
%	}
%	fin.close();
%	// debug();
%
%	MaxMinNorm();
%	// ZScoreNorm();
%}
%
%void KNN::ZScoreNorm()
%{
%	for (int j = 0; j < col; j++)
%	{
%		double avg = 0;
%		for (int i = 0; i < row; i++)
%			avg += dataSet[i].data[j];
%		avg /= (double)row;
%
%		double sig = 0;
%		for (int i = 0; i < row; i++)
%			sig += (dataSet[i].data[j] - avg) * (dataSet[i].data[j] - avg);
%		sig /= (double)row;
%		sig = sqrt(sig);
%
%		for (int i = 0; i < row; i++)
%			dataSet[i].data[j] = (dataSet[i].data[j] - avg) / sig;
%	}
%}
%
%void KNN::MaxMinNorm()
%{
%	for (int j = 0; j < col; j++)
%	{
%		double maxx = max(dataSet[0].data[j], dataSet[1].data[j]);
%		double minx = min(dataSet[0].data[j], dataSet[1].data[j]);
%		for (int i = 0; i < row; i++)
%		{
%			//if (dataSet[i].data[j] > maxx) maxx = dataSet[i].data[j];
%			//else if (dataSet[i].data[j] < minx) minx = dataSet[i].data[j];
%			maxx = max(maxx, dataSet[i].data[j]);
%			minx = min(minx, dataSet[i].data[j]);
%		}
%		for (int i = 0; i < row; i++)
%			dataSet[i].data[j] = (double)(dataSet[i].data[j] - minx) / (double)(maxx - minx);
%	}
%}
%
%double KNN::dis(const Node &A, const Node &B)
%{
%	double ret = 0;
%	for (int i = 0; i < col; i++)
%		ret += (A.data[i] - B.data[i]) * (A.data[i] - B.data[i]);
%	return sqrt(ret);
%}
%
%__global__ void DisKernel(double *traindata, double *testdata, double *dis, int pitch, int N, int D)
%{
%	int tid = blockIdx.x;
%	if (tid < N)
%	{
%		double temp = 0;
%		double sum = 0;
%		for (int i = 0; i < D; i++)
%		{
%			temp = *((double*)((char*)traindata + tid * pitch) + i) - testdata[i];
%			sum += temp * temp;
%		}
%		dis[tid] = sum;
%	}
%}
%
%void KNN::CalDis()
%{
%	vec.clear();
%
%	//cudaMemset(GTestData, 0, col * sizeof(double));
%	cudaMemset(GDis, 0, trainRow * sizeof(double));
%
%	cudaMemcpy(GTestData, dataTest.data, col * sizeof(double), cudaMemcpyHostToDevice);
%
%	DisKernel << <trainRow, 1 >> >(GTrainData, GTestData, GDis, pitch_d, trainRow, col);
%
%	cudaMemcpy(distance, GDis, trainRow * sizeof(double), cudaMemcpyDeviceToHost);
%
%	for (int i = testNum; i < row; i++)
%		vec.push_back(make_pair(i, distance[i - testNum]));
%
%}
%
%string KNN::MaxFreqLabel()
%{
%	CalDis();
%	sort(vec.begin(), vec.end(), CMP());
%
%	mp.clear();
%	for (int i = 0; i < therK; i++)
%		mp[dataSet[vec[i].first].label]++;
%
%	string ret;
%	int cnt = 0;
%	for (auto ite = mp.begin(); ite != mp.end(); ite++)
%	{
%		if (ite->second > cnt)
%		{
%			cnt = ite->second;
%			ret = ite->first;
%		}
%	}
%
%	return ret;
%}
%
%void KNN::knn()
%{
%	CUDAInit();
%	cout << "Test data num: " << testNum << endl;
%
%	int cnt = 0;
%	for (int i = 0; i < testNum; i++)
%	{
%		dataTest = dataSet[i];
%		if (MaxFreqLabel() == dataTest.label)
%			cnt++;
%	}
%
%	//cout << cnt << " " << testNum << endl;
%
%	cout << "Accuracy Rate: " << (double)cnt / (double)testNum << endl;
%}
%
%void KNN::CUDAInit()
%{
%	trainRow = row - testNum;
%	for (int i = testNum; i < row; i++)
%	{
%		for (int j = 0; j < col; j++)
%			trainmtx[i - testNum][j] = dataSet[i].data[j];
%	}
%
%	pitch_h = col*sizeof(double);
%
%	cudaMallocPitch(&GTrainData, &pitch_d, col * sizeof(double), trainRow);
%	cudaMemset(GTrainData, 0, trainRow * col * sizeof(double));
%	cudaMemcpy2D(GTrainData, pitch_d, trainmtx, pitch_h, col * sizeof(double), trainRow, cudaMemcpyHostToDevice);
%
%	distance = new double[trainRow];
%
%	cudaMalloc(&GTestData, col * sizeof(double));
%	cudaMalloc(&GDis, trainRow * sizeof(double));
%}
%
%void KNN::debug()
%{
%	ofstream fout;
%	fout.open("debug.txt");
%	for (int i = 0; i < row; i++)
%	{
%		for (int j = 0; j < col; j++)
%			fout << dataSet[i].data[j] << "\t";
%		fout << dataSet[i].label << endl;
%	}
%}
%
%KNN knn;
%
%void init()
%{
%	knn.init(7, 45222, 8, "allTypeC.txt");
%}
%void input()
%{
%	knn.input();
%}
%void debug()
%{
%	//
%}
%void solve()
%{
%	clock_t st, ed;
%	st = clock();
%	knn.knn();
%	ed = clock();
%	cout << "Time: " << ed - st << endl;
%}
%void output()
%{
%	//
%}
%int main()
%{
%	init();
%	input();
%	solve();
%	output();
%
%	return 0;
%}
%\end{lstlisting}
%
%\subsection{}
%\begin{lstlisting}[language=python]
%a
%\end{lstlisting}
%
%\subsection{}
%\begin{lstlisting}[language=python]
%from sklearn.linear_model import LogisticRegression
%from sklearn import cross_validation,metrics
%from sklearn.svm import SVC
%from sklearn.cross_validation import ShuffleSplit
%from sklearn.learning_curve import learning_curve
%from sklearn.grid_search import GridSearchCV
%from sklearn.naive_bayes import GaussianNB
%from sklearn import preprocessing
%
%import pickle,sys,os
%import time
%from datetime import datetime
%from pprint import pprint
%import numpy as np
%
%if __name__ == '__main__':
%
%	# print("all.pik")
%	print("allNew.pik")
%	print(datetime.now())
%	# fp = open("all.pik","rb")
%	fp = open("allNew.pik","rb")
%	allData = pickle.load(fp)
%	m = len(allData)
%	n = len(allData[0])
%	print("orgidata:",m,n)
%	for i in range(0,m):
%		try:
%			for j in range(0,n):
%				allData[i][j] = float(allData[i][j])
%		except Exception:
%			print("i,",i)
%			pprint(allData[i])
%	allD = np.array(allData)
%	label = allD[:,-1]
%	allData = allD[:,:-1]
%	# minMaxScaler = preprocessing.MinMaxScaler()
%	# allData = minMaxScaler.fit_transform(allData)
%	L = np.shape(allData)[0]
%	print("items: ",L)
%	iterN = 10
%	ss = ShuffleSplit(L,iterN,test_size=0.1,random_state=0)
%	cnt = 0
%	LogAcc = []
%	# Set the parameters by cross-validation
%	log_tuned_parameters = [{'penalty': ['l1'],'C': [1, 5, 10, 100]},
%	                    {'penalty': ['l2'], 'C': [1, 5 ,10, 100]}]
%
%	svm_tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.5],'C': [1, 5, 10, 100]},
%							{'kernel': ['linear'], 'gamma': [0.5],'C': [1, 5, 10, 100]}]
%
%	for trainId,testId in ss:
%		'''
%		  Get train data,label and test data,label accroding to shufflesplit
%		'''
%		cnt+=1
%		trainVec = allData[trainId]
%		trainLabel = label[trainId]
%		testVec = allData[testId]
%		testLabel = label[testId]
%
%		print("######iter %d#####" %cnt)
%
%		###logistic #####
%		print("logistic regression:")
%		logS = time.clock()
%		logisticRegression = LogisticRegression()
%		score = "f1_micro"
%		cv =ShuffleSplit(len(trainVec),3,test_size=0.1,random_state=10)
%		clf =  GridSearchCV(logisticRegression, log_tuned_parameters, cv=cv, n_jobs =1,
%	                       scoring='%s' % score)
%		clf.fit(trainVec,trainLabel)
%		print(clf.best_params_)
%		predict = clf.predict(testVec)
%
%		# logisticRegression.fit(trainVec,trainLabel)
%		# predict = logisticRegression.predict(testVec)
%
%		accuracy = metrics.accuracy_score(testLabel,predict)
%		recall = metrics.recall_score(testLabel,predict,pos_label=None,average='micro')
%		# recall = [ sum(recall)/2.0,recall[1] ]
%		# print("accuracy,recall: ",accuracy,recall[0],recall[-1])
%		print("accuracy,recall: ",accuracy,recall)
%		print(metrics.classification_report(testLabel,predict))
%		print("confusion_matrix:")
%		cm = metrics.confusion_matrix(testLabel,predict)
%		print(cm)
%		logE = time.clock()
%		print("Logistic Regression time: {0}s".format(logE-logS) )
%
%		###logistic #####
%		print("GaussianNB :")
%		logS = time.clock()
%		NaiveB = LogisticRegression()
%		score = "f1_micro"
%		cv =ShuffleSplit(len(trainVec),3,test_size=0.1,random_state=10)
%		clf =  GridSearchCV(NaiveB, {}, cv=cv, n_jobs=1,
%	                       scoring='%s' % score)
%		clf.fit(trainVec,trainLabel)
%		print(clf.best_params_)
%		predict = clf.predict(testVec)
%
%		# logisticRegression.fit(trainVec,trainLabel)
%		# predict = logisticRegression.predict(testVec)
%
%		accuracy = metrics.accuracy_score(testLabel,predict)
%		recall = metrics.recall_score(testLabel,predict,pos_label=None,average='micro')
%		# recall = [ sum(recall)/2.0,recall[1] ]
%		# print("accuracy,recall: ",accuracy,recall[0],recall[-1])
%		print("accuracy,recall: ",accuracy,recall)
%		print(metrics.classification_report(testLabel,predict))
%		print("confusion_matrix:")
%		cm = metrics.confusion_matrix(testLabel,predict)
%		print(cm)
%		logE = time.clock()
%		print("GaussianNB time: {0}s".format(logE-logS) )
%
%
%		## svm #####
%		print("\nsvm:")
%		svmTS = time.clock()
%		svmKernelway = 'rbf'
%		svrrbf = SVC(kernel=svmKernelway,C=10,gamma=0.5)
%		# print(type(testVec),np.shape(testVec))
%
%		score = "f1_micro"
%		cv =ShuffleSplit(len(trainVec),2,test_size=0.1,random_state=10)
%		clf =  GridSearchCV(SVC(C=1), svm_tuned_parameters, cv=cv, n_jobs = 2,
%	                       scoring='%s' % score)
%
%		clf.fit(trainVec,trainLabel)
%		print(clf.best_params_)
%		testLabel = clf.predict(testVec)
%
%		# svrrbf.fit(trainVec,trainLabel)
%		# predict = svrrbf.predict(testVec)
%		accuracy = metrics.accuracy_score(testLabel,predict)
%		recall = metrics.recall_score(testLabel,predict,pos_label=None,average='micro')
%		# recall = [ sum(recall)/2.0,recall[1] ]
%		print("accuracy,recall: ",accuracy,recall)
%		print(metrics.classification_report(testLabel,predict))
%		print("confusion_matrix:")
%		cm = metrics.confusion_matrix(testLabel,predict)
%		print(cm)
%		svmTE = time.clock()
%		print("svm time: {0}s".format(svmTE-svmTS) )
%
%# # ####logistic regression#####
%# print("logistic regression:")
%# logisticRegression = LogisticRegression()
%# accuracy = cross_validation.cross_val_score(logisticRegression,train,label,scoring='accuracy',cv=10)
%# print("accuracy:",accuracy)
%# print("accuracy mean:",accuracy.mean())
%# recalls = cross_validation.cross_val_score(logisticRegression,train,label,scoring='recall_micro',cv=10)
%# print("recall:",recalls)
%# print("recall mean:",recalls.mean())
%
%# # ####svm####
%# print("svm:")
%# svmKernelway = 'linear'
%# svrrbf = SVC(kernel=svmKernelway,C=10,gamma=0.5)
%# accuracy = cross_validation.cross_val_score(svrrbf,train,label,scoring='accuracy',cv=10)
%# print("accuracy:",accuracy)
%# print("accuracy mean:",accuracy.mean())
%# recalls = cross_validation.cross_val_score(svrrbf,train,label,scoring='recall_micro',cv=10)
%# print("recall:",recalls)
%# print("recall mean:",recalls.mean())
%# print("\n")
%\end{lstlisting}

\end{document}
